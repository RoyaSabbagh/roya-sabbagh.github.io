<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Course_projects on </title>
    <link>https://royasabbagh.github.io/course_project/</link>
    <description>Recent content in Course_projects on </description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2018 Roya Sabbagh Novin</copyright>
    <lastBuildDate>Sun, 27 Nov 2016 00:00:00 -0700</lastBuildDate>
    
	<atom:link href="https://royasabbagh.github.io/course_project/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Motion Planning in Danger Zones</title>
      <link>https://royasabbagh.github.io/course_project/rrt/</link>
      <pubDate>Sun, 27 Nov 2016 00:00:00 -0700</pubDate>
      
      <guid>https://royasabbagh.github.io/course_project/rrt/</guid>
      <description>In this project, a RRT-based method to cope with quadrotor path planning problem in damaged structures is proposed. First, a Graphic User Interface (GUI) was developed to define the environment conveniently. Then, RRT was used as basic path generating method. Besides, the limit on acceleration of the robot was considered to meet its dynamic constraints, and cost function was proposed to avoid danger zones in the environment. Next, B-spline method was implemented to achieve the smoothing of the final path.</description>
    </item>
    
    <item>
      <title>Object Recognition</title>
      <link>https://royasabbagh.github.io/course_project/object_recognition/</link>
      <pubDate>Sun, 27 Nov 2016 00:00:00 -0700</pubDate>
      
      <guid>https://royasabbagh.github.io/course_project/object_recognition/</guid>
      <description>This project is focused on single object recognition and classification. The MNIST database is used as the image dataset. A single layer CNN is developed which consists of a convolutional layer and a pooling layer. The results obtained for this database is test accuracy of around 97.44% which is comparable with the results found in the literature. In the next step, the single layer CNN is expanded to a three-layer CNN for the CIFAR-10 database in order to classify each image to one of the 10 known categories.</description>
    </item>
    
    <item>
      <title>Mobile Robot Visual Localization and 3D Map Generation</title>
      <link>https://royasabbagh.github.io/course_project/mobile_localization/</link>
      <pubDate>Fri, 27 May 2016 00:00:00 -0600</pubDate>
      
      <guid>https://royasabbagh.github.io/course_project/mobile_localization/</guid>
      <description>Course: ME CS 6320- 3D Computer Vision
In this project, we will implement a Visual Odometry method on a Roomba iCreate 2 which is equipped with a Microsoft Kinect RGB-D sensor for localization and map generation. Although the mobile robot is moving on a 2D plane (ground), the localization and map generation algorithms are for 3D environments, so the final results are in 3D. At the end, we will evaluate our approach by comparing resulted localization with results from robot odometry, LIDAR sensor localization, or GPS data.</description>
    </item>
    
  </channel>
</rss>